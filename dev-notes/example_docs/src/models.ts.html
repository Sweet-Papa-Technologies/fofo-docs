<h1 id="srcmodelstsfofodocs">src/models.ts - fofo-docs</h1>
<p><strong>Summary:</strong> This code defines a set of LLM models with their names, model identifiers, backend platforms (Ollama, OpenAI, Vertex), and optional context sizes. It provides a list of recommended models for an application, allowing users to select and utilize different LLM models based on their needs.</p>
<ul>
<li><strong>File Location:</strong> ./src/models.ts</li>
<li><strong>Language:</strong> language: TypeScript </li>
</ul>
<h2 id="tableofcontents">Table of Contents</h2>
<ul>
<li><a href="#functions">functions</a></li>
<li><a href="#variables">variables</a></li>
<li><a href="#types">types</a></li>
<li><a href="#imports">imports</a></li>
<li><a href="#interfaces">interfaces</a></li>
</ul>
<h2 id="functions">functions</h2>
<h2 id="getmodelbackendfunction">### üîß getModelBackend - FUNCTION</h2>
<p><strong>Description:</strong> This function retrieves the backend type for a given model name from the MODEL_MODES array.</p>
<p><strong>Code Snippet:</strong></p>
<pre><code class="typescript language-typescript">const getModelBackend = (selectedModel: string) =&gt; {
  const model = MODEL_MODES.find((m) =&gt; m.model === selectedModel);
  if (model) {
    return model.backend as llm_modes;
  }
  throw new Error("Model not found");
};
</code></pre>
<ul>
<li><strong>Line:</strong> Could Not Verify Line</li>
<li><strong>Location:</strong> models.ts (./src/models.ts)</li>
<li><strong>Exported:</strong> true</li>
<li><strong>Private:</strong> false</li>
<li><strong>Async:</strong> false</li>
</ul>
<h6 id="functionparameters">Function Parameters:</h6>
<ul>
<li><strong>selectedModel</strong> (string): The name of the model to retrieve the backend for. 
Example: "phi3"</li>
</ul>
<h6 id="functionreturns">Function Returns:</h6>
<ul>
<li><strong>Type:</strong> llm_modes</li>
<li><strong>Description:</strong> The backend type for the given model, or an error if the model is not found.</li>
<li><strong>Example:</strong> "OLLAMA"</li>
</ul>
<h6 id="annotationscomments">Annotations / Comments:</h6>
<ul>
<li><strong>Purpose:</strong> The <code>getModelBackend</code> function is responsible for retrieving the backend type associated with a given model name. It searches the <code>MODEL_MODES</code> array for a model with a matching name and returns its backend type. If no matching model is found, it throws an error.</li>
<li><strong>Parameters:</strong> - <code>selectedModel</code>: A string representing the name of the model to retrieve the backend for. For example, "phi3".</li>
<li><strong>Returns:</strong> - <code>llm_modes</code>: A string representing the backend type for the given model. Possible values include "OLLAMA", "VERTEX", or "OPENAI". If the model is not found, an error is thrown.</li>
<li><strong>Usage Example:</strong> </li>
</ul>
<pre><code class="typescript language-typescript">const backend = getModelBackend("phi3");
console.log(backend); // Output: "OLLAMA"
</code></pre>
<ul>
<li><strong>Edge Cases:</strong> If the provided <code>selectedModel</code> does not exist in the <code>MODEL_MODES</code> array, the function throws an error.</li>
<li><strong>Dependencies:</strong> - <code>MODEL_MODES</code> array: This array contains the list of available models and their backend types.</li>
</ul>
<h2 id="variables">variables</h2>
<h2 id="modelsvariable">### üßÆ MODELS - VARIABLE</h2>
<p><strong>Description:</strong> An array of objects that represent different LLM models.</p>
<p><strong>Code Snippet:</strong></p>
<pre><code class="typescript language-typescript">export const MODELS:llmModel[] = [

];
</code></pre>
<ul>
<li><strong>Line:</strong> 19</li>
<li><strong>Location:</strong> models.ts (./src/models.ts)</li>
<li><strong>Exported:</strong> true</li>
<li><strong>Private:</strong> false</li>
</ul>
<h6 id="annotationscomments-1">Annotations / Comments:</h6>
<ul>
<li><strong>Purpose:</strong> This variable defines an array of objects that represent different LLM models available for use in the application.</li>
<li><strong>Usage Example:</strong> </li>
</ul>
<pre><code class="typescript language-typescript">const selectedModel = MODELS.find(model =&gt; model.name === 'phi3');
</code></pre>
<h2 id="model_modes_basevariable">### üßÆ MODEL<em>MODES</em>BASE - VARIABLE</h2>
<p><strong>Description:</strong> An array of objects that represent the recommended LLM models for the application.</p>
<p><strong>Code Snippet:</strong></p>
<pre><code class="typescript language-typescript">export const MODEL_MODES_BASE:llmModel[] = [
  {
    "name": "deepseek-coder-v2:16b-lite-instruct-q6_K",
    "model": "deepseek-coder-v2:16b-lite-instruct-q6_K",
    "backend": "OLLAMA"
  },
  {
    "name": "gemma2:27b",
    "model": "gemma2:27b",
    "backend": "OLLAMA",
    context: 32000
  },
  {
    "name": "gemma2:27b-instruct-q6_K",
    "model": "gemma2:27b-instruct-q6_K",
    "backend": "OLLAMA",
    context: 32000
  },
  {
    "name": "yi:34b",
    "model": "yi:34b",
    "backend": "OLLAMA",
    context: 32000
  },
  {
    "name": "codestral:22b-v0.1-q5_0",
    "model": "codestral:22b-v0.1-q5_0",
    "backend": "OLLAMA",
    context: 32000
  },
    {
      "name": "codestral:22b-v0.1-q5_1",
      "model": "codestral:22b-v0.1-q5_1",
      "backend": "OLLAMA",
      context: 32000
    },

    {
      "name": "granite-code:34b",
      "model": "granite-code:34b",
      "backend": "OLLAMA"
    },
    {
      "name": "phi3:14b-medium-128k-instruct-q5_1",
      "model": "phi3:14b-medium-128k-instruct-q5_1",
      "backend": "OLLAMA"
    }
    ,
    {
      name: "phi3:14b-medium-4k-instruct-q6_K",
      model: "phi3:14b-medium-4k-instruct-q6_K",
      backend: "OLLAMA"
    },
    {
      name: "qwen:32b-chat-v1.5-q4_K_M",
      model: "qwen:32b-chat-v1.5-q4_K_M",
      backend: "OLLAMA"
    },
    {
      name: "mixtral:8x7b-instruct-v0.1-q3_K_L",
      model: "mixtral:8x7b-instruct-v0.1-q3_K_L",
      backend: "OLLAMA"
    },
    {
      name: "qwen:32b-chat-v1.5-q4_0",
      model: "qwen:32b-chat-v1.5-q4_0",
      backend: "OLLAMA"
    },
    {
      name: "codeqwen:7b-code-v1.5-q8_0",
      model: "codeqwen:7b-code-v1.5-q8_0",
      backend: "OLLAMA"
    },
    {
      name: "llama3-chatqa:8b-v1.5-fp16",
      model: "llama3-chatqa:8b-v1.5-fp16",
      backend: "OLLAMA"
    },
    {
      name: "qwen:32b-text-v1.5-q4_0",
      model: "qwen:32b-text-v1.5-q4_0",
      backend: "OLLAMA"
    },
    {
      name: "gpt-4o",
      model: "gpt-4o",
      backend: "OPENAI"
    },
    {
      name: "phi3:3.8b-mini-instruct-4k-fp16",
      model: "phi3:3.8b-mini-instruct-4k-fp16",
      backend: "OLLAMA",
    },
    {
      name: "llama3-gradient:8b-instruct-1048k-q6_K",
      model: "llama3-gradient:8b-instruct-1048k-q6_K",
      backend: "OLLAMA",
    },
    {
      name: "phi3",
      model: "phi3",
      backend: "OLLAMA",
    },
    {
      name: "dolphin-llama3:8b-v2.9-fp16",
      model: "dolphin-llama3:8b-v2.9-fp16",
      backend: "OLLAMA",
    },
    {
      name: "codechat-bison",
      model: "codechat-bison",
      backend: "VERTEX",
    },
    {
      name: "codechat-bison-32k",
      model: "codechat-bison-32k",
      backend: "VERTEX",
    },
    {
      name: "gemini-1.5-flash-001",
      model: "gemini-1.5-flash-001",
      backend: "VERTEX",
    },
    {
      name: "gemini-1.5-flash-preview-0514",
      model: "gemini-1.5-flash-preview-0514",
      backend: "VERTEX",
    },
    {
      name: "gemini-1.5-pro-preview-0514",
      model: "gemini-1.5-pro-preview-0514",
      backend: "VERTEX",
    },
  ];
</code></pre>
<ul>
<li><strong>Line:</strong> 29</li>
<li><strong>Location:</strong> models.ts (./src/models.ts)</li>
<li><strong>Exported:</strong> true</li>
<li><strong>Private:</strong> false</li>
</ul>
<h6 id="annotationscomments-2">Annotations / Comments:</h6>
<ul>
<li><strong>Purpose:</strong> This variable defines an array of objects representing recommended LLM models for the application. Each object contains the model's name, model identifier, backend platform (Ollama, OpenAI, Vertex), and optional context size.</li>
<li><strong>Usage Example:</strong> </li>
</ul>
<pre><code class="typescript language-typescript">const model = MODEL_MODES_BASE.find(m =&gt; m.model === 'yi:34b');
</code></pre>
<h2 id="types">types</h2>
<h2 id="llmbackendmodetype">### üè∑Ô∏è llmBackendMode - TYPE</h2>
<p><strong>Description:</strong> Type alias for the backend mode of the LLM.</p>
<p><strong>Code Snippet:</strong></p>
<pre><code class="typescript language-typescript">export type llmBackendMode = 'OLLAMA' | 'OPENAI' | 'VERTEX';
</code></pre>
<ul>
<li><strong>Line:</strong> 2</li>
<li><strong>Location:</strong> models.ts (./src/models.ts)</li>
<li><strong>Exported:</strong> Could Not Determine</li>
<li><strong>Private:</strong> Could Not Determine</li>
</ul>
<h6 id="annotationscomments-3">Annotations / Comments:</h6>
<ul>
<li><strong>Purpose:</strong> This type alias defines the possible backend modes for the LLM, which are 'OLLAMA', 'OPENAI', and 'VERTEX'.</li>
<li><strong>Usage Example:</strong> </li>
</ul>
<pre><code class="typescript language-typescript">const backendMode: llmBackendMode = 'OLLAMA';
</code></pre>
<h2 id="llmmodeltype">### üè∑Ô∏è llmModel - TYPE</h2>
<p><strong>Description:</strong> Type interface for the LLM model.</p>
<p><strong>Code Snippet:</strong></p>
<pre><code class="typescript language-typescript">export type llmModel = {
    name: string;
    model: string;
    backend: llmBackendMode;
    context?: number
}
</code></pre>
<ul>
<li><strong>Line:</strong> 3</li>
<li><strong>Location:</strong> models.ts (./src/models.ts)</li>
<li><strong>Exported:</strong> Could Not Determine</li>
<li><strong>Private:</strong> Could Not Determine</li>
</ul>
<h6 id="annotationscomments-4">Annotations / Comments:</h6>
<ul>
<li><strong>Purpose:</strong> This type interface defines the structure for an LLM model, specifying its name, model identifier, backend platform, and optional context size.</li>
<li><strong>Usage Example:</strong> </li>
</ul>
<pre><code class="typescript language-typescript">const myModel: llmModel = {
  name: "gpt-3.5-turbo",
  model: "gpt-3.5-turbo",
  backend: "OPENAI",
  context: 4096
};
</code></pre>
<h2 id="imports">imports</h2>
<h2 id="loggerimport">### üì• ./logger - IMPORT</h2>
<p><strong>Description:</strong> Imports the logger module from the current directory.</p>
<p><strong>Code Snippet:</strong></p>
<pre><code class="typescript language-typescript">import "./logger";
</code></pre>
<ul>
<li><strong>Line:</strong> 1</li>
<li><strong>Location:</strong> models.ts (./src/models.ts)</li>
<li><strong>Exported:</strong> Could Not Determine</li>
<li><strong>Private:</strong> Could Not Determine</li>
</ul>
<h6 id="annotationscomments-5">Annotations / Comments:</h6>
<ul>
<li><strong>Purpose:</strong> This line imports the <code>logger</code> module from the current directory, likely to use its logging functionality within the <code>models.ts</code> file.</li>
<li><strong>Dependencies:</strong> The <code>logger</code> module from the current directory.</li>
</ul>
<h2 id="interfaces">interfaces</h2>
<h2 id="llmmodelinterface">### üåâ llmModel - INTERFACE</h2>
<p><strong>Description:</strong> Interface for defining LLM models.</p>
<p><strong>Code Snippet:</strong></p>
<pre><code class="typescript language-typescript">export type llmModel = {
    name: string;
    model: string;
    backend: llmBackendMode;
    context?: number
}
</code></pre>
<ul>
<li><strong>Line:</strong> 3</li>
<li><strong>Location:</strong> models.ts (./src/models.ts)</li>
<li><strong>Exported:</strong> Could Not Determine</li>
<li><strong>Private:</strong> Could Not Determine</li>
</ul>
<h6 id="annotationscomments-6">Annotations / Comments:</h6>
<ul>
<li><strong>Purpose:</strong> This interface defines the structure for representing different Large Language Models (LLMs) used in the application. It specifies the name, model identifier, backend platform (Ollama, OpenAI, Vertex), and optional context size for each LLM.</li>
<li><strong>Usage Example:</strong> </li>
</ul>
<pre><code class="typescript language-typescript">const myModel: llmModel = {
  name: "phi3",
  model: "phi3",
  backend: "OLLAMA"
};
</code></pre>